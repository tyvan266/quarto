[
  {
    "objectID": "posts/conditional-aggregation-p1/post.html",
    "href": "posts/conditional-aggregation-p1/post.html",
    "title": "Exercises in Counting",
    "section": "",
    "text": "Life tables are common in actuarial sciences, bio-medical studies, and more generally in the field of survival analysis. They are used to track a dynamic population over time, one where individuals may enter and leave at different times, and they are important for computing at-risk rates. In this post, we will demonstrate how to compute simple life table summaries.\nSuppose we have patient data on enrollment start times and censoring or death times. For simplicity, instead of using R date types, we will represent start and end times as numeric integers. This set up is very similar to our first post on gaps and islands.\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(ggplot2)\n\npatient_fup_data &lt;- \n  tibble(\n    ID             = 1:200, \n    startFollowUp  = c(sample(1:200, 200, replace = TRUE)),\n    endDeathCensor = startFollowUp + sample(30:100, 200, replace = TRUE)  \n  )\n\n\nhead(patient_fup_data, n = 10) |&gt; kable()\n\n\n\n\nID\nstartFollowUp\nendDeathCensor\n\n\n\n\n1\n105\n145\n\n\n2\n167\n222\n\n\n3\n56\n125\n\n\n4\n116\n170\n\n\n5\n87\n163\n\n\n6\n177\n211\n\n\n7\n103\n133\n\n\n8\n57\n144\n\n\n9\n23\n61\n\n\n10\n12\n87\n\n\n\n\n\nRoughly, we can visualize when new patients enter and when they leave (due to death or censoring or exiting our risk set). Each segment represents a unique patient and their time under observation.\n\n\nCode\nggplot(patient_fup_data |&gt; filter(ID &lt;= 30)) +\n  geom_linerange(\n    aes(xmin = startFollowUp, xmax = endDeathCensor, y = ID)\n    , linewidth = 1) +\n  \n  theme_bw() +\n  xlab(\"Entry / Exit\") +\n  ylab(\"Patient ID\") + \n  \n  theme(\n    axis.text.x = element_text(size = 16, angle = 45),\n    strip.text = element_text(size = 18), \n    axis.title.x = element_text(size = 24),\n    axis.title.y = element_text(size = 24),\n    legend.position = 'right'\n  )\n\n\n\n\n\nObservation time intervals by patients\n\n\n\n\nOur goal is to count over time the current number of patients under observation.\n\n\nOur first solution will highlight the merging operators left_join and cross_join (also known as cartesian join).\n\n\n\n\n\n\nWarning\n\n\n\nCartesian joins can be computationally intensive and slow!\n\n\nFirst, we pre-define a grid of time points over which to compute our counts. Suppose we assess counts every 60 time units. In the code below, the cartesian join will match every row in the left table (our reference grid points) with every row in the right table (our patient data). Then we make an indicator for if the patient’s observation window intersects the grid point, and group by time and sum them up:\n\nsurvival_grid &lt;- tibble(xtime = seq(0, 500, 60))\n\nsurv_grid_xpatients &lt;- survival_grid |&gt; cross_join(patient_fup_data)\n\nsurvival_grid_counts &lt;- \nsurv_grid_xpatients |&gt; \n  mutate(count = startFollowUp &lt;= xtime & endDeathCensor &gt;= xtime) |&gt; \n  group_by(xtime) |&gt; \n  summarise(count = sum(count))\n\n\nhead(survival_grid_counts) |&gt; kable()\n\n\n\n\nxtime\ncount\n\n\n\n\n0\n0\n\n\n60\n52\n\n\n120\n75\n\n\n180\n69\n\n\n240\n26\n\n\n300\n0\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWithin the cartesian join, observe each patient can only contribute one count towards each grid time point.\n\n\n\n\n\nIn this method, we do a rolling count moving through each patient. Eventually we subset on unique entry times where a change in the total population can possibly occur. In particular, at each entry time when one or more new patients enroll, we add up the total number of enrollees (a simple row count works here as we cycle through the patient data set). But we must adjust at each entry the total number lost to death or censoring up to that point. As more than one patient may enter at the same time, the final counts appear last when grouped by entry times and provided we start counting in order beginning from the earliest entry.\n\nN_under_observation &lt;- \npatient_fup_data |&gt; \n  arrange(startFollowUp) |&gt; \n  \n  mutate(\n    Nenroll = row_number(), \n    Nlost   = map_dbl(startFollowUp, \\(.x) sum(endDeathCensor &lt;= .x)), \n    N_obs   = Nenroll - Nlost \n  ) |&gt; \n  \n  group_by(startFollowUp) |&gt; \n  slice(n()) \n\nFinally, we visualize the change in our patient population size over time.\n\n\nCode\nggplot(N_under_observation, aes(x = startFollowUp, y = N_obs)) + \n  geom_step(size = 1) + \n  xlab(\"Time\") + \n  ylab(\"Number under observation\") + \n  theme_bw() + \n  theme(\n    axis.text.x = element_text(size = 16, angle = 45),\n    strip.text = element_text(size = 18), \n    axis.title.x = element_text(size = 24),\n    axis.title.y = element_text(size = 24),\n    legend.position = 'right'\n  )\n\n\n\n\n\nPopulation size over time\n\n\n\n\nThere is a caveat with our solution. We are not accounting for exit times that occur between two successive entry times. This might be okay as the total lost will be factored in at the later entry date, but if the time until a new enrollment is long, it can be misleading of the population size at risk, particularly if there was a large drop-off somewhere in-between. Related to this, we are also not seeing when everyone exits (population at risk becomes zero). These should be kept in mind when interpreting and communicating the information presented in the figure."
  },
  {
    "objectID": "posts/conditional-aggregation-p1/post.html#grid-method",
    "href": "posts/conditional-aggregation-p1/post.html#grid-method",
    "title": "Exercises in Counting",
    "section": "",
    "text": "Our first solution will highlight the merging operators left_join and cross_join (also known as cartesian join).\n\n\n\n\n\n\nWarning\n\n\n\nCartesian joins can be computationally intensive and slow!\n\n\nFirst, we pre-define a grid of time points over which to compute our counts. Suppose we assess counts every 60 time units. In the code below, the cartesian join will match every row in the left table (our reference grid points) with every row in the right table (our patient data). Then we make an indicator for if the patient’s observation window intersects the grid point, and group by time and sum them up:\n\nsurvival_grid &lt;- tibble(xtime = seq(0, 500, 60))\n\nsurv_grid_xpatients &lt;- survival_grid |&gt; cross_join(patient_fup_data)\n\nsurvival_grid_counts &lt;- \nsurv_grid_xpatients |&gt; \n  mutate(count = startFollowUp &lt;= xtime & endDeathCensor &gt;= xtime) |&gt; \n  group_by(xtime) |&gt; \n  summarise(count = sum(count))\n\n\nhead(survival_grid_counts) |&gt; kable()\n\n\n\n\nxtime\ncount\n\n\n\n\n0\n0\n\n\n60\n52\n\n\n120\n75\n\n\n180\n69\n\n\n240\n26\n\n\n300\n0\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWithin the cartesian join, observe each patient can only contribute one count towards each grid time point."
  },
  {
    "objectID": "posts/conditional-aggregation-p1/post.html#approximately-continuous-method-map-to-the-rescue",
    "href": "posts/conditional-aggregation-p1/post.html#approximately-continuous-method-map-to-the-rescue",
    "title": "Exercises in Counting",
    "section": "",
    "text": "In this method, we do a rolling count moving through each patient. Eventually we subset on unique entry times where a change in the total population can possibly occur. In particular, at each entry time when one or more new patients enroll, we add up the total number of enrollees (a simple row count works here as we cycle through the patient data set). But we must adjust at each entry the total number lost to death or censoring up to that point. As more than one patient may enter at the same time, the final counts appear last when grouped by entry times and provided we start counting in order beginning from the earliest entry.\n\nN_under_observation &lt;- \npatient_fup_data |&gt; \n  arrange(startFollowUp) |&gt; \n  \n  mutate(\n    Nenroll = row_number(), \n    Nlost   = map_dbl(startFollowUp, \\(.x) sum(endDeathCensor &lt;= .x)), \n    N_obs   = Nenroll - Nlost \n  ) |&gt; \n  \n  group_by(startFollowUp) |&gt; \n  slice(n()) \n\nFinally, we visualize the change in our patient population size over time.\n\n\nCode\nggplot(N_under_observation, aes(x = startFollowUp, y = N_obs)) + \n  geom_step(size = 1) + \n  xlab(\"Time\") + \n  ylab(\"Number under observation\") + \n  theme_bw() + \n  theme(\n    axis.text.x = element_text(size = 16, angle = 45),\n    strip.text = element_text(size = 18), \n    axis.title.x = element_text(size = 24),\n    axis.title.y = element_text(size = 24),\n    legend.position = 'right'\n  )\n\n\n\n\n\nPopulation size over time\n\n\n\n\nThere is a caveat with our solution. We are not accounting for exit times that occur between two successive entry times. This might be okay as the total lost will be factored in at the later entry date, but if the time until a new enrollment is long, it can be misleading of the population size at risk, particularly if there was a large drop-off somewhere in-between. Related to this, we are also not seeing when everyone exits (population at risk becomes zero). These should be kept in mind when interpreting and communicating the information presented in the figure."
  },
  {
    "objectID": "posts/gaps-and-islands/post.html",
    "href": "posts/gaps-and-islands/post.html",
    "title": "The Gaps and Islands Problem",
    "section": "",
    "text": "The gaps and islands problem arises from analyzing duration data as commonly captured in databases. Duration data are events recorded with a start datetime and end datetime. Rows in the table correspond to unique events that someone is experiencing, or during which times the event is active. As individuals ID may have one or more event activity, such data are often stored in long-form as date ranges or intervals.\n\n\n\n\n\n\nNote\n\n\n\nFor simplicity, we assume only one type of event under study — the data can be generalized to include an event-type field.\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\nset.seed(12345)\n\n# 10 individuals\nID &lt;- 1:10\n\n# 3 to 7 events per person \nn_events &lt;- sample(3:7, 10, replace = TRUE)\n\nduration_data &lt;- tibble(ID = rep(ID, n_events)) |&gt; \n  mutate(\n    start_date = \n      sample(seq(as.Date('2022-01-01'), as.Date('2022-12-31'), by = \"day\"), n(), replace = TRUE),\n    # 3 days - 2 months episodes \n    end_date = start_date + sample(3:60, n(), replace = TRUE)   \n  ) |&gt; \n  arrange(ID, start_date) |&gt; \n  group_by(ID) |&gt; \n  mutate(eventID = row_number())\n\n\nFor example:\n\nhead(duration_data, n=15) |&gt; kable()\n\n\n\n\nID\nstart_date\nend_date\neventID\n\n\n\n\n1\n2022-02-09\n2022-03-06\n1\n\n\n1\n2022-07-31\n2022-08-09\n2\n\n\n1\n2022-09-14\n2022-11-10\n3\n\n\n1\n2022-10-13\n2022-11-10\n4\n\n\n1\n2022-10-21\n2022-11-23\n5\n\n\n2\n2022-01-12\n2022-02-20\n1\n\n\n2\n2022-01-14\n2022-02-08\n2\n\n\n2\n2022-09-16\n2022-11-03\n3\n\n\n2\n2022-11-24\n2022-12-11\n4\n\n\n3\n2022-01-16\n2022-01-23\n1\n\n\n3\n2022-03-21\n2022-05-05\n2\n\n\n3\n2022-04-16\n2022-05-23\n3\n\n\n3\n2022-05-21\n2022-06-04\n4\n\n\n3\n2022-05-28\n2022-06-23\n5\n\n\n3\n2022-07-11\n2022-08-08\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo reveal the code how to simulate the demo data, click the Code text to expand above.\n\n\nHere, ID 1 experiences 5 distinct eventIDs, with the first one occurring on 2/9 and lasting for 25 days until 3/6. The key observation is that eventID 4 overlaps with eventID 5 within ID 1, where event 5 beginning on 10/21 starts before event 4 has ended on 11/10.\n\n\nVisualizing the date ranges of event occurrences is useful. We plot all individual event date ranges colored by ID, supplying a position_dodge so overlaps are revealed (instead of lying right on top one another).\n\n\nCode\nlibrary(ggplot2)\n\nggplot(duration_data |&gt; filter(ID &lt;= 5),\n       aes(y = factor(ID), xmin = start_date, xmax = end_date, color = factor(ID))) +\n  geom_linerange(position = position_dodge2(width = .5), linewidth = 2) +\n  theme_bw() + \n  xlab(\"Start/ End Dates\") + ylab(\"Individual ID\") +\n  scale_x_date(date_breaks = \"1 month\") + \n  theme(\n    axis.text.x = element_text(size = 16, angle = 45),\n    axis.text.y = element_text(size = 16),\n    axis.title.x = element_text(size = 24),\n    axis.title.y = element_text(size = 24),\n    legend.position = 'none'\n  )\n\n\n\n\n\nDuration patterns by subject\n\n\n\n\n\n\n\nGiven the set-up, how do we want to collapse the data? Our input data is structured as one row per date range per person. The desired output data is one row per coverage interval per person. By coverage, we mean a continuous block of time, defined by a start date and end date, during which the individual is experiencing one or more active events.\n\n\n\n\n\n\nNote\n\n\n\nCoverage intervals are referred to as islands. In-between time periods when there are no events occurring (no date ranges, overlapping or otherwise) are called gaps (holes).\n\n\nFor our demo data, within ID 2, there are 4 eventIDs, but just 3 coverage intervals (islands): some event is active between\n\n2022-01-12 through 2022-02-08 (where the date range of event 2 completely overlaps and within the duration time of event 1)\n2022-09-16 through 2022-11-03 (duration of eventID 3)\n2022-11-24 through 2022-12-11 (duration of last eventID 4)\n\nHence our desired output table will comprise 3 rows for ID 2, with start and end times as described above for each coverage interval. Likewise, by visual inspection, ID 3, who experienced 6 events, will have only 3 coverage intervals as well, with one particularly long coverage interval (of back-to-back events) beginning on 2022-03-21 and finally abating on 2022-06-23.\n\n\n\nExamining the gaps and islands solution can reveal hidden assumptions about data processes or quality issues you might not be aware of. For example, perhaps certain overlap patterns violate assumptions about event frequencies, or some duration lengths and patterns are not physically possible. Typically, the data captured does not reflect the complete story.\nConsider medical databases housing patient medication prescriptions. The table captures PatientID, DispensedDate, and DaysSupply, from which an EndDate is derived representing date of supply depletion relative to dispense time. The data as entered by pharmacists or business administrators for billing purposes or inventory management is only one part of the story, especially if our goal is to understand how patients are treated.\nFor example, suppose we observe multiple overlapping prescriptions for the same medication type (overlapping date ranges), but perhaps differing dose levels. It is not clear whether the patient has been instructed to take both prescriptions simultaneously (effective dose increase), or perhaps to switch to the lower dose before finishing the other prescription. We have missing contextual information! And, if we observe overlapping date ranges among different medication types, we may find new or surprising treatment regimes warranting investigation.\nIf we know what we see is impossible, and we have high assurance of data validity (they were not entered erroneously), we investigate further. Perhaps, it turns out a subset of prescriptions were returned-to-stock and were never picked up by the patient, requiring a check on another data field that we were not aware was being captured. With data in the wild, as we’re removed further away from the source of data origination, incomplete code-books, inaccurate code-books, or non-existent code-books represent the norm.\n\n\n\n\n\n\nNote\n\n\n\nUnless we already know what to look for, generalizing how we describe event patterns can help reveal new information requiring follow-up."
  },
  {
    "objectID": "posts/gaps-and-islands/post.html#visualization",
    "href": "posts/gaps-and-islands/post.html#visualization",
    "title": "The Gaps and Islands Problem",
    "section": "",
    "text": "Visualizing the date ranges of event occurrences is useful. We plot all individual event date ranges colored by ID, supplying a position_dodge so overlaps are revealed (instead of lying right on top one another).\n\n\nCode\nlibrary(ggplot2)\n\nggplot(duration_data |&gt; filter(ID &lt;= 5),\n       aes(y = factor(ID), xmin = start_date, xmax = end_date, color = factor(ID))) +\n  geom_linerange(position = position_dodge2(width = .5), linewidth = 2) +\n  theme_bw() + \n  xlab(\"Start/ End Dates\") + ylab(\"Individual ID\") +\n  scale_x_date(date_breaks = \"1 month\") + \n  theme(\n    axis.text.x = element_text(size = 16, angle = 45),\n    axis.text.y = element_text(size = 16),\n    axis.title.x = element_text(size = 24),\n    axis.title.y = element_text(size = 24),\n    legend.position = 'none'\n  )\n\n\n\n\n\nDuration patterns by subject"
  },
  {
    "objectID": "posts/gaps-and-islands/post.html#what-do-we-wish-to-solve-for",
    "href": "posts/gaps-and-islands/post.html#what-do-we-wish-to-solve-for",
    "title": "The Gaps and Islands Problem",
    "section": "",
    "text": "Given the set-up, how do we want to collapse the data? Our input data is structured as one row per date range per person. The desired output data is one row per coverage interval per person. By coverage, we mean a continuous block of time, defined by a start date and end date, during which the individual is experiencing one or more active events.\n\n\n\n\n\n\nNote\n\n\n\nCoverage intervals are referred to as islands. In-between time periods when there are no events occurring (no date ranges, overlapping or otherwise) are called gaps (holes).\n\n\nFor our demo data, within ID 2, there are 4 eventIDs, but just 3 coverage intervals (islands): some event is active between\n\n2022-01-12 through 2022-02-08 (where the date range of event 2 completely overlaps and within the duration time of event 1)\n2022-09-16 through 2022-11-03 (duration of eventID 3)\n2022-11-24 through 2022-12-11 (duration of last eventID 4)\n\nHence our desired output table will comprise 3 rows for ID 2, with start and end times as described above for each coverage interval. Likewise, by visual inspection, ID 3, who experienced 6 events, will have only 3 coverage intervals as well, with one particularly long coverage interval (of back-to-back events) beginning on 2022-03-21 and finally abating on 2022-06-23."
  },
  {
    "objectID": "posts/gaps-and-islands/post.html#application-in-medicine",
    "href": "posts/gaps-and-islands/post.html#application-in-medicine",
    "title": "The Gaps and Islands Problem",
    "section": "",
    "text": "Examining the gaps and islands solution can reveal hidden assumptions about data processes or quality issues you might not be aware of. For example, perhaps certain overlap patterns violate assumptions about event frequencies, or some duration lengths and patterns are not physically possible. Typically, the data captured does not reflect the complete story.\nConsider medical databases housing patient medication prescriptions. The table captures PatientID, DispensedDate, and DaysSupply, from which an EndDate is derived representing date of supply depletion relative to dispense time. The data as entered by pharmacists or business administrators for billing purposes or inventory management is only one part of the story, especially if our goal is to understand how patients are treated.\nFor example, suppose we observe multiple overlapping prescriptions for the same medication type (overlapping date ranges), but perhaps differing dose levels. It is not clear whether the patient has been instructed to take both prescriptions simultaneously (effective dose increase), or perhaps to switch to the lower dose before finishing the other prescription. We have missing contextual information! And, if we observe overlapping date ranges among different medication types, we may find new or surprising treatment regimes warranting investigation.\nIf we know what we see is impossible, and we have high assurance of data validity (they were not entered erroneously), we investigate further. Perhaps, it turns out a subset of prescriptions were returned-to-stock and were never picked up by the patient, requiring a check on another data field that we were not aware was being captured. With data in the wild, as we’re removed further away from the source of data origination, incomplete code-books, inaccurate code-books, or non-existent code-books represent the norm.\n\n\n\n\n\n\nNote\n\n\n\nUnless we already know what to look for, generalizing how we describe event patterns can help reveal new information requiring follow-up."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "A collection of posts on data management principles using R for the aspiring programmer."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Sieving Data, A Quarto Blog",
    "section": "",
    "text": "Exercises in Counting\n\n\n\n\n\nApplication to survival analysis\n\n\n\n\n\nApr 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDoing iteration with map (when vectorized functions are not enough)\n\n\n\n\n\nApplication to window functions and computing composite scores with missing values\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Gaps and Islands Problem\n\n\n\n\n\nAlso known as the overlapping-date-ranges problem, where we study duration data to identify coverage intervals (or, holes)\n\n\n\n\n\nOct 4, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/intro-purrr-map/post.html",
    "href": "posts/intro-purrr-map/post.html",
    "title": "Doing iteration with map (when vectorized functions are not enough)",
    "section": "",
    "text": "Functions in the tidyverse suite of libraries are typically vectorized. They know how to process vector (or list) arguments and return a vector (list) output. This means we get looping-behavior for free without having to explicitly program the looping structure.\n\n\n\n\n\n\nNote\n\n\n\nIn R, vectors are special cases of lists where all elements in the list are of the same atomic type (dbl, char, logical, etc.) — i.e., when they do not hold further objects (or other lists).\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\nset.seed(12345)\n\ndemo_data &lt;- \n  tribble(\n    ~ \"X\", ~ \"Y\", ~ \"Z\",\n    3, 8, 5,\n    2, 4, 7,\n    6, 3, 8,\n    1, 5, 2\n  )\n\n\nFor example, consider the follow demo data:\n\nkable(demo_data)\n\n\n\n\nX\nY\nZ\n\n\n\n\n3\n8\n5\n\n\n2\n4\n7\n\n\n6\n3\n8\n\n\n1\n5\n2\n\n\n\n\n\n\n\nSuppose we want to create a new variable W on the data.frame defined as follows:\n\nif X is less than Y, then W = Z\notherwise, W = 2 * Z\n\nWe can achieve this using the vectorized if_else function wrapped around the mutate method for column creation. Let’s also create the sum of X and Y into V\n\ndemo_data |&gt; \n  mutate(\n    W = if_else(X &lt; Y, Z, 2 * Z),\n    V = X + Y\n  ) |&gt;\n        kable()\n\n\n\n\nX\nY\nZ\nW\nV\n\n\n\n\n3\n8\n5\n5\n11\n\n\n2\n4\n7\n7\n6\n\n\n6\n3\n8\n16\n9\n\n\n1\n5\n2\n2\n6"
  },
  {
    "objectID": "posts/intro-purrr-map/post.html#thresholding-and-counting",
    "href": "posts/intro-purrr-map/post.html#thresholding-and-counting",
    "title": "Doing iteration with map (when vectorized functions are not enough)",
    "section": "Thresholding and counting",
    "text": "Thresholding and counting\nWe demonstrate with a simple application to thresholding. Suppose we have a list of values X, and a smaller list of thresholding values T\n\nlibrary(purrr)\n\nX = c(3, 7, 10, 3, 14, 20, 11, 27)\nT = c(3, 10, 15, 22)\n\nWe want to know, at some threshold of interest, how many values in X fall below the threshold. For the first threshold value 3, we can answer using sum with the vectorized predicate &lt;\n\nX &lt; 3\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nsum(X &lt; 3)\n\n[1] 0\n\n\nTo execute this for all values in our threshold list, we use map_dbl — the first argument is the list of input values we want to loop over (threshold values), and the second argument is an anonymous function over X and parameterized by the threshold argument .t. The final result will match the dimensions of the argument list that we want to run the function over (of size 4).\n\nmap_dbl(T, \\(.t) sum(X &lt; .t))\n\n[1] 0 3 6 7\n\n\n\n\n\n\n\n\nTip\n\n\n\nAn anonymous function, one without a name, is defined in R by \\(.argument) { .body }. Functions are first-class citizens, meaning they can be passed around as arguments into yet other functions."
  },
  {
    "objectID": "posts/intro-purrr-map/post.html#composite-scores-with-missing-values",
    "href": "posts/intro-purrr-map/post.html#composite-scores-with-missing-values",
    "title": "Doing iteration with map (when vectorized functions are not enough)",
    "section": "Composite scores with missing values",
    "text": "Composite scores with missing values\nWe want to compute and track some composite score at each visit. We would like to use the value taken at the visit time, but we’re willing to impute with the closest value 30 days prior. Our composite measure is \\(X + Y\\).\n\npatient_visit_data |&gt; \n1  group_by(ID) |&gt;\n  mutate(\n2    XplusY = map_dbl(VisitDate,\n3      \\(.v)\n        last(\n4          X[VisitDate &lt;= .v & VisitDate &gt;= .v - 30], na_rm = TRUE)\n        +\n        last(\n          Y[VisitDate &lt;= .v & VisitDate &gt;= .v - 30], na_rm = TRUE))\n    \n    ) |&gt; \n  \n5  filter(ID == 2 | ID == 8) |&gt;\n  kable(n = 20)\n\n\n1\n\nTransform the data by processing the variables within patient\n\n\n2\n\nUsing map_dbl to make our composite score of X and Y. Its first argument is the VisitDate column — the list of values we want to loop through or use as input values to our user-defined function\n\n3\n\nThe head of our user-defined function, which will be represented as an anonymous function passed entirely into the second argument of map_dbl. The function is parameterized by the dummy variable .v that will be drawn from the input list\n\n4\n\nThe body of our user-defined anonymous function. Observe it’s a function of the environment variables X and VisitDate scoped from the data.frame context. Note VisitDate used here is independent and different from outer use also in map_dbl!\n\n5\n\nExamine the outputs for patients 2 and 8\n\n\n\n\n\n\n\nID\nVisitDate\nX\nY\nZ\nXplusY\n\n\n\n\n2\n2022-01-12\n24\n92\n48\n116\n\n\n2\n2022-01-15\n34\n113\n47\n147\n\n\n2\n2022-02-20\nNA\n103\n50\nNA\n\n\n2\n2022-03-03\nNA\nNA\nNA\nNA\n\n\n2\n2022-03-15\nNA\n93\n48\nNA\n\n\n2\n2022-04-13\nNA\nNA\n52\nNA\n\n\n2\n2022-05-04\n9\nNA\n55\nNA\n\n\n2\n2022-05-13\n31\nNA\nNA\nNA\n\n\n2\n2022-06-25\n28\n88\nNA\n116\n\n\n2\n2022-06-28\n44\nNA\n59\n132\n\n\n8\n2022-01-02\nNA\nNA\nNA\nNA\n\n\n8\n2022-01-11\n24\n74\n56\n98\n\n\n8\n2022-01-25\n28\nNA\n41\n102\n\n\n8\n2022-02-03\n40\n93\n59\n133\n\n\n8\n2022-02-07\nNA\nNA\n50\n133\n\n\n8\n2022-04-16\n5\n66\n52\n96\n\n\n8\n2022-04-16\n30\nNA\n49\n96\n\n\n8\n2022-05-12\n42\n93\n49\n135\n\n\n8\n2022-05-15\nNA\n102\n52\n144\n\n\n8\n2022-05-23\n27\nNA\nNA\n129\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe function last has an argument called na_rm with default value False. Here, we want the last available measurement recorded when sorted by descending date — which will correspond to the most recent record."
  },
  {
    "objectID": "posts/intro-purrr-map/post.html#map-for-window-aggregations",
    "href": "posts/intro-purrr-map/post.html#map-for-window-aggregations",
    "title": "Doing iteration with map (when vectorized functions are not enough)",
    "section": "Map for window aggregations",
    "text": "Map for window aggregations\nSuppose our next set of tasks are to compute at each scheduled visit\n\nThe average all X values taken within the last 30 days (rolling average of available values)\nThe maximum Y value recorded so far, but excluded the current Y\nThe current value of Z, or the most recent one recorded within the last 10 days\n\n\npatient_visit_with_window_summary &lt;- patient_visit_data |&gt; \n  \n  group_by(ID) |&gt; \n  mutate(\n    \n1    X_30day_average =\n      map_dbl(VisitDate, \n        \\(.v) mean(X[VisitDate &lt;= .v & VisitDate &gt;= .v - 30], na.rm = TRUE)),\n    \n2    Y_max =\n      map_dbl(VisitDate, \n        \\(.v) max(Y[VisitDate &lt;= .v], na.rm = TRUE)),\n    \n3    Z_recent =\n      map_dbl(VisitDate, \n        \\(.v) last(Z[VisitDate &lt;= .v & VisitDate &gt;= .v - 10], na_rm = TRUE))\n    )\n\n\n1\n\n30 day average of X\n\n2\n\nCumulative maximum of Y\n\n3\n\nMost recent Z within the last 10 days\n\n\n\n\nFor each variable, we use map_dbl where the first argument is the VisitDate column (processed within patient) that we want to iterate over as inputs. The user-defined anonymous functions passed into the second argument of map_dbl are functions over the environment variables X, Y, or Z, subset by VisitDate (also scoped separately from the data.frame context) and the dummy variable .v. They eventually reduce the filtered variables using the aggregation functions mean, max, and last.\n\n\nCode\npatient_visit_with_window_summary |&gt; filter(ID == 3 | ID == 7) |&gt;\n  kable(n = 20) |&gt; column_spec(1:7, border_left = TRUE, border_right = TRUE)\n\n\n\n\n\nID\nVisitDate\nX\nY\nZ\nX_30day_average\nY_max\nZ_recent\n\n\n\n\n3\n2022-01-03\n39\nNA\n53\n39.00000\n-Inf\n53\n\n\n3\n2022-01-07\nNA\n105\n53\n39.00000\n105\n53\n\n\n3\n2022-02-24\n16\n78\n49\n16.00000\n105\n49\n\n\n3\n2022-02-25\n35\n109\n50\n25.50000\n109\n50\n\n\n3\n2022-03-27\n48\nNA\n40\n41.50000\n109\n40\n\n\n3\n2022-04-04\n30\n94\n47\n39.00000\n109\n47\n\n\n3\n2022-04-08\n37\n63\nNA\n38.33333\n109\n47\n\n\n3\n2022-04-09\n28\nNA\n52\n35.75000\n109\n52\n\n\n3\n2022-05-25\n36\n103\nNA\n36.00000\n109\nNA\n\n\n3\n2022-06-01\nNA\n89\nNA\n36.00000\n109\nNA\n\n\n7\n2022-01-18\n35\n91\n55\n35.00000\n91\n55\n\n\n7\n2022-02-01\n24\n44\n46\n29.50000\n91\n46\n\n\n7\n2022-02-27\n35\n73\nNA\n29.50000\n91\nNA\n\n\n7\n2022-04-06\n26\n90\n56\n26.00000\n91\n56\n\n\n7\n2022-04-26\n24\n102\n51\n25.00000\n102\n51\n\n\n7\n2022-05-01\nNA\n88\nNA\n25.00000\n102\n51\n\n\n7\n2022-05-12\nNA\n92\nNA\n24.00000\n102\nNA\n\n\n7\n2022-05-15\n56\n90\nNA\n40.00000\n102\nNA\n\n\n7\n2022-05-28\nNA\n64\n54\n56.00000\n102\n54\n\n\n7\n2022-06-16\nNA\n89\n46\nNaN\n102\n46\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen supplying arguments for na.rm or na_rm, always provide the full values TRUE or FALSE instead of using the abbreviated shortcut T or F.\n\n\nTo help spot-check the results (particularly for the rolling 30-day look-back averages), use the general map to get a list of values meeting the condition at each visit (list of lists!) prior to aggregation by mean. For Z_recent, you may want to validate that the 10-day look-back is working as expected.\n\npatient_visit_with_window_summary &lt;- patient_visit_data |&gt; \n  \n  group_by(ID) |&gt;\n  \n  mutate(\n    \n1    Xs_30day = map(VisitDate, \\(.v) X[VisitDate &lt;= .v & VisitDate &gt;= .v - 30]),\n    \n    X_30day_average = \n      map_dbl(VisitDate, \n        \\(.v) mean(X[VisitDate &lt;= .v & VisitDate &gt;= .v - 30], na.rm = TRUE)),\n    \n    Y_max = \n      map_dbl(VisitDate, \n        \\(.v) max(Y[VisitDate &lt;= .v], na.rm = TRUE)),\n    \n    Z_recent = \n      map_dbl(VisitDate, \n        \\(.v) last(Z[VisitDate &lt;= .v & VisitDate &gt;= .v - 10], na_rm = TRUE))\n    \n    ) \n\n\n1\n\nNow our anonymous function filters X using the VisitDate column and the index visit date .v, but does not summarize the vector further. Instead of returning a dbl value at each execution (like map_dbl), it must return back an entire list at each call. Hence, in the end map returns a vector of lists.\n\n\n\n\n\n\nCode\npatient_visit_with_window_summary |&gt; filter(ID == 3 | ID == 7) |&gt;\n    kable(n = 20) |&gt; column_spec(1:8, border_left = TRUE, border_right = TRUE)\n\n\n\n\n\nID\nVisitDate\nX\nY\nZ\nXs_30day\nX_30day_average\nY_max\nZ_recent\n\n\n\n\n3\n2022-01-03\n39\nNA\n53\n39\n39.00000\n-Inf\n53\n\n\n3\n2022-01-07\nNA\n105\n53\n39, NA\n39.00000\n105\n53\n\n\n3\n2022-02-24\n16\n78\n49\n16\n16.00000\n105\n49\n\n\n3\n2022-02-25\n35\n109\n50\n16, 35\n25.50000\n109\n50\n\n\n3\n2022-03-27\n48\nNA\n40\n35, 48\n41.50000\n109\n40\n\n\n3\n2022-04-04\n30\n94\n47\n48, 30\n39.00000\n109\n47\n\n\n3\n2022-04-08\n37\n63\nNA\n48, 30, 37\n38.33333\n109\n47\n\n\n3\n2022-04-09\n28\nNA\n52\n48, 30, 37, 28\n35.75000\n109\n52\n\n\n3\n2022-05-25\n36\n103\nNA\n36\n36.00000\n109\nNA\n\n\n3\n2022-06-01\nNA\n89\nNA\n36, NA\n36.00000\n109\nNA\n\n\n7\n2022-01-18\n35\n91\n55\n35\n35.00000\n91\n55\n\n\n7\n2022-02-01\n24\n44\n46\n35, 24\n29.50000\n91\n46\n\n\n7\n2022-02-27\n35\n73\nNA\n24, 35\n29.50000\n91\nNA\n\n\n7\n2022-04-06\n26\n90\n56\n26\n26.00000\n91\n56\n\n\n7\n2022-04-26\n24\n102\n51\n26, 24\n25.00000\n102\n51\n\n\n7\n2022-05-01\nNA\n88\nNA\n26, 24, NA\n25.00000\n102\n51\n\n\n7\n2022-05-12\nNA\n92\nNA\n24, NA, NA\n24.00000\n102\nNA\n\n\n7\n2022-05-15\n56\n90\nNA\n24, NA, NA, 56\n40.00000\n102\nNA\n\n\n7\n2022-05-28\nNA\n64\n54\nNA, NA, 56, NA\n56.00000\n102\n54\n\n\n7\n2022-06-16\nNA\n89\n46\nNA, NA\nNaN\n102\n46"
  }
]