[
  {
    "objectID": "posts/conditional-aggregation-p1/post.html",
    "href": "posts/conditional-aggregation-p1/post.html",
    "title": "When vectorized functions are not enough: using map for iteration",
    "section": "",
    "text": "Basics of mapping functions\nOrdinarily, dplyr functions operate on entire vectors of data, or lists when accessing data.frame column names\n\n\n\n\n\n\nImportant\n\n\n\nIn tidyverse functions, what we may perceive as row-wise operations, functions acting on the scalar values taken from two or more columns, are actually vectorized functions that are acting on the entire set of column values.\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\nset.seed(12345)\n\ndemo_data &lt;- \n  tribble(\n    ~ \"X\", ~ \"Y\", ~ \"Z\",\n    3, 8, 5,\n    2, 4, 7,\n    6, 3, 8,\n    1, 5, 2\n  )\n\n\nConsider the follow demo data:\n\nkable(demo_data)\n\n\n\n\nX\nY\nZ\n\n\n\n\n3\n8\n5\n\n\n2\n4\n7\n\n\n6\n3\n8\n\n\n1\n5\n2\n\n\n\n\n\n\n\nWhen we create a new column via mutate say for the sum of X and Y, or even X + 1, it’s tempting to think the operation is happening row by row as the return list is being created.\n\ndemo_data |&gt; \n  mutate(\n    Sum    = X + Y, \n    Xplus1 = X + 1\n  ) |&gt; kable() \n\n\n\n\nX\nY\nZ\nSum\nXplus1\n\n\n\n\n3\n8\n5\n11\n4\n\n\n2\n4\n7\n6\n3\n\n\n6\n3\n8\n9\n7\n\n\n1\n5\n2\n6\n2\n\n\n\n\n\n\n\nBut the return columns are list types themselves, and it’s more useful to remember that our functions are taking in one set of list types arguments as returning another.\nBut instead, we may actually want to proceed row-wise at least along one column, while processing the list column of another field. We want to manipulate columns while using values and scalars from other columns. For example, one column act as a list of arguments or parameters, and we want to use the current value as a scalar input to aggregate over another column taken a list type.\nInstead, consider the following task: Create a new column W where for each value of X, we sum all values of Y but after having multiplied them by the current X scalar value.\nWe can get this to work by hand:\n\ndemo_data$X[1]  \n\n[1] 3\n\ndemo_data$X[1] * demo_data$Y\n\n[1] 24 12  9 15\n\nsum(demo_data$X[1] * demo_data$Y) \n\n[1] 60\n\n\nIf our brains continue to think things work row-by-row, we may be led astray with an incorrect first attempt:\n\ndemo_data |&gt; \n  mutate(\n    W = sum(X * Y)) |&gt; kable() \n\n\n\n\nX\nY\nZ\nW\n\n\n\n\n3\n8\n5\n55\n\n\n2\n4\n7\n55\n\n\n6\n3\n8\n55\n\n\n1\n5\n2\n55\n\n\n\n\n\n\n\nBecause W return type must be a list, but the function operations are aggregating over other list types, it just repeats the same value along each row. But we don’t want it to act on X as a vector.\n\n\n\n\n\n\nTip\n\n\n\nA scalar is a one-dimensional data type (compared to matrices, lists, data.frames, objects, etc.). Simply, a scalar is a singular value or number, and that’s all — 2 is scalar, but the vector containing the single element two, [2], is not.\n\n\nIntroduce map_dbl. The first argument is a list, and the second argument is a function of the scalar value extracted along the list, and along with other data lists taken from the data.frame environment. Hence it will return a new vector, one dbl result for processing each value in the list input, and this satisfies the return type that mutate is returning an entire vector output matching the dimensions of the original data.frame\n\ndemo_data |&gt; \n  mutate(\n    W = map_dbl(X, \\(.x) sum(.x * Y))) |&gt; kable() \n\n\n\n\nX\nY\nZ\nW\n\n\n\n\n3\n8\n5\n60\n\n\n2\n4\n7\n40\n\n\n6\n3\n8\n120\n\n\n1\n5\n2\n20\n\n\n\n\n\n\n\nOur anonymous function uses the entire Y list as input to it’s processing. The .x argument must be a scalar because it is drawn from X within the use of map_dbl, which will run the function for each value of X, putting the result into a return list W\n\n\nWarm-up: Leave-one-out comparisons\nWithin group, compare to means of the others…\nLet’s generate some data by group, where maybe observations different from the rest. drawn from a mixture of distributions of sort.\n\n# 100 rnorm \nx &lt;- rnorm(90)\ny &lt;- rep(5, 10)\n\ndemo_grp_data &lt;-\n  tibble(\n    ID = rep(1:10, 10),\n    v = sample(c(x,y), replace = F)\n  )\n\nFor each observation within ID, we want to compute the absolute difference between the observation under consideration, with the mean of the rest of the observations in the group.\n\ndemo_grp_data |&gt; \n  group_by(ID) |&gt; \n  mutate(\n    X = map_dbl(v, \\(.x) (sum(v) - .x)/(length(v) - 1)) \n  ) |&gt; \n  \n  arrange(ID) \n\n# A tibble: 100 × 3\n# Groups:   ID [10]\n      ID       v     X\n   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1     1 -1.34   0.503\n 2     1  0.0546 0.348\n 3     1  0.517  0.296\n 4     1  0.888  0.255\n 5     1  1.46   0.192\n 6     1  0.943  0.249\n 7     1  0.854  0.259\n 8     1  0.254  0.326\n 9     1  0.620  0.285\n10     1 -1.06   0.472\n# ℹ 90 more rows\n\n\n\n\nBrain teaser level:\nOverlapping intervals again, with reference to current: say, find all concomittant drugs\nFake data example. This is similar to the overlapping date ranges problem, but here we each interval act as its own reference or index point from which we want to compare. Instead of using proper datetypes, we use numeric Begin and End for simplicity.\n\ndemo_data &lt;- \n  tribble(\n    ~ \"ID\", ~ \"Begin\", ~ \"End\", ~ \"Type\",\n       1      ,12       , 18     ,\"A\"   ,\n       1      ,20       , 25     ,\"B\"   ,\n       1      ,23       , 30     ,\"C\"   ,\n       2      ,7        , 13     ,\"A\"   ,\n       2      ,12       , 20     ,\"B\"   ,\n       3      ,18       , 28     ,\"A\"   ,\n       3      ,20       , 33     ,\"B\"   ,\n       3      ,31       , 42     ,\"C\"   ,\n       3      ,44       , 50     ,\"D\"   ,\n       4      ,10       , 25     ,\"A\"   ,\n       4      ,15       , 19     ,\"B\"   ,\n       4      ,21       , 30     ,\"C\"   ,\n       5      ,3        , 8      ,\"A\"   ,\n       5      ,7        , 15     ,\"B\"   ,\n       5      ,13       , 22     ,\"C\"   ,\n       5      ,19       , 26     ,\"D\"   ,\n       5      ,25       , 33     ,\"E\"\n  ) |&gt; \n  mutate(ID = factor(ID))\n\nSimilar interval range data\n\n\nCode\nggplot(demo_data, aes(color = Type)) +\n  geom_linerange(\n    aes(xmin = Begin, xmax = End, y = 0)\n    , linewidth = 2\n    , position = position_dodge2(width = .5)) + \n  scale_color_brewer(palette = \"Set1\") + \n  \n  xlab(\"Begin/ End\") + ylab(\"ID Panels\") +\n  scale_x_continuous(breaks = seq(0,50,5)) + \n\n  theme_bw() +\n  \n  facet_grid(ID ~ .) + \n  \n  theme(\n    axis.text.x = element_text(size = 16, angle = 45),\n    axis.text.y = element_blank(),\n    strip.text = element_text(size = 18), \n    axis.title.x = element_text(size = 24),\n    axis.title.y = element_text(size = 24),\n    legend.position = 'right'\n  )\n\n\n\n\n\nSolution\nFor each interval, we loop through their Start and End and subset the intervals from Type list that satisfying the corresponding conditions on their starts and ends.\n\ndemo_data |&gt; \n  group_by(ID) |&gt; \n  arrange(ID, Begin) |&gt; \n  mutate(\n    findOverlaps = \n      map2_chr(Begin, End, \\(.x, .y) str_c(Type[!(.x &gt;= End | .y &lt;= Begin)], collapse = \" & \"))\n  )\n\n# A tibble: 17 × 5\n# Groups:   ID [5]\n   ID    Begin   End Type  findOverlaps\n   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n 1 1        12    18 A     A           \n 2 1        20    25 B     B & C       \n 3 1        23    30 C     B & C       \n 4 2         7    13 A     A & B       \n 5 2        12    20 B     A & B       \n 6 3        18    28 A     A & B       \n 7 3        20    33 B     A & B & C   \n 8 3        31    42 C     B & C       \n 9 3        44    50 D     D           \n10 4        10    25 A     A & B & C   \n11 4        15    19 B     A & B       \n12 4        21    30 C     A & C       \n13 5         3     8 A     A & B       \n14 5         7    15 B     A & B & C   \n15 5        13    22 C     B & C & D   \n16 5        19    26 D     C & D & E   \n17 5        25    33 E     D & E       \n\n\n\n\nIndustrial Problem Solving\nGiven patients data on enrollment start, (beginning of follow-up observation) and censoring or death date, we want to compute a timeseries graph showing the total N under observation at a given time.\n\npatient_fup_data &lt;- \n  tibble(\n    ID             = 1:200, \n    startFollowUp  = c(sample(1:200, 100, replace = TRUE), sample(300:500, 100, replace = TRUE)),\n    endDeathCensor = startFollowUp + sample(5:100, 200, replace = TRUE)  \n  )\n\n\nkable(patient_fup_data, n = 10)\n\n\n\n\nID\nstartFollowUp\nendDeathCensor\n\n\n\n\n1\n127\n194\n\n\n2\n78\n116\n\n\n3\n135\n148\n\n\n4\n92\n112\n\n\n5\n164\n242\n\n\n6\n17\n107\n\n\n7\n33\n89\n\n\n8\n177\n226\n\n\n9\n182\n242\n\n\n10\n95\n170\n\n\n11\n149\n240\n\n\n12\n195\n200\n\n\n13\n135\n180\n\n\n14\n54\n116\n\n\n15\n15\n29\n\n\n16\n149\n221\n\n\n17\n135\n174\n\n\n18\n73\n163\n\n\n19\n193\n266\n\n\n20\n58\n87\n\n\n21\n126\n133\n\n\n22\n175\n222\n\n\n23\n104\n109\n\n\n24\n146\n189\n\n\n25\n158\n175\n\n\n26\n89\n98\n\n\n27\n75\n112\n\n\n28\n85\n97\n\n\n29\n86\n163\n\n\n30\n187\n285\n\n\n31\n58\n146\n\n\n32\n70\n95\n\n\n33\n123\n223\n\n\n34\n83\n88\n\n\n35\n24\n40\n\n\n36\n24\n54\n\n\n37\n181\n211\n\n\n38\n162\n258\n\n\n39\n7\n92\n\n\n40\n77\n135\n\n\n41\n80\n107\n\n\n42\n39\n100\n\n\n43\n86\n163\n\n\n44\n53\n82\n\n\n45\n162\n256\n\n\n46\n161\n173\n\n\n47\n9\n89\n\n\n48\n160\n168\n\n\n49\n133\n196\n\n\n50\n150\n178\n\n\n51\n200\n224\n\n\n52\n52\n118\n\n\n53\n72\n146\n\n\n54\n68\n121\n\n\n55\n171\n203\n\n\n56\n56\n134\n\n\n57\n167\n208\n\n\n58\n186\n285\n\n\n59\n118\n198\n\n\n60\n2\n78\n\n\n61\n37\n53\n\n\n62\n10\n47\n\n\n63\n112\n175\n\n\n64\n92\n159\n\n\n65\n66\n105\n\n\n66\n83\n143\n\n\n67\n114\n157\n\n\n68\n189\n223\n\n\n69\n19\n64\n\n\n70\n161\n222\n\n\n71\n176\n214\n\n\n72\n16\n62\n\n\n73\n24\n53\n\n\n74\n186\n191\n\n\n75\n138\n173\n\n\n76\n113\n158\n\n\n77\n142\n216\n\n\n78\n154\n176\n\n\n79\n41\n139\n\n\n80\n93\n160\n\n\n81\n44\n114\n\n\n82\n92\n167\n\n\n83\n115\n130\n\n\n84\n86\n170\n\n\n85\n159\n235\n\n\n86\n9\n27\n\n\n87\n107\n197\n\n\n88\n59\n130\n\n\n89\n121\n152\n\n\n90\n181\n233\n\n\n91\n73\n117\n\n\n92\n148\n192\n\n\n93\n59\n71\n\n\n94\n51\n119\n\n\n95\n57\n107\n\n\n96\n10\n49\n\n\n97\n6\n102\n\n\n98\n169\n219\n\n\n99\n32\n105\n\n\n100\n184\n210\n\n\n101\n348\n398\n\n\n102\n322\n344\n\n\n103\n460\n548\n\n\n104\n437\n486\n\n\n105\n405\n410\n\n\n106\n499\n572\n\n\n107\n450\n533\n\n\n108\n362\n385\n\n\n109\n361\n377\n\n\n110\n435\n503\n\n\n111\n434\n449\n\n\n112\n300\n387\n\n\n113\n466\n477\n\n\n114\n412\n481\n\n\n115\n493\n545\n\n\n116\n500\n590\n\n\n117\n390\n474\n\n\n118\n364\n452\n\n\n119\n492\n585\n\n\n120\n460\n469\n\n\n121\n477\n517\n\n\n122\n311\n348\n\n\n123\n333\n352\n\n\n124\n445\n472\n\n\n125\n326\n387\n\n\n126\n472\n556\n\n\n127\n304\n395\n\n\n128\n468\n553\n\n\n129\n454\n463\n\n\n130\n500\n550\n\n\n131\n495\n515\n\n\n132\n340\n429\n\n\n133\n371\n464\n\n\n134\n452\n505\n\n\n135\n499\n595\n\n\n136\n359\n375\n\n\n137\n412\n429\n\n\n138\n487\n502\n\n\n139\n352\n393\n\n\n140\n311\n397\n\n\n141\n452\n504\n\n\n142\n340\n427\n\n\n143\n416\n448\n\n\n144\n334\n385\n\n\n145\n302\n334\n\n\n146\n400\n448\n\n\n147\n475\n483\n\n\n148\n356\n439\n\n\n149\n407\n496\n\n\n150\n379\n435\n\n\n151\n340\n347\n\n\n152\n360\n446\n\n\n153\n470\n509\n\n\n154\n333\n379\n\n\n155\n324\n418\n\n\n156\n446\n479\n\n\n157\n315\n366\n\n\n158\n358\n395\n\n\n159\n496\n583\n\n\n160\n408\n463\n\n\n161\n410\n440\n\n\n162\n434\n445\n\n\n163\n462\n512\n\n\n164\n382\n444\n\n\n165\n362\n458\n\n\n166\n419\n480\n\n\n167\n336\n370\n\n\n168\n366\n411\n\n\n169\n422\n440\n\n\n170\n488\n533\n\n\n171\n417\n429\n\n\n172\n384\n445\n\n\n173\n372\n423\n\n\n174\n450\n495\n\n\n175\n479\n569\n\n\n176\n409\n415\n\n\n177\n421\n478\n\n\n178\n388\n456\n\n\n179\n464\n509\n\n\n180\n434\n524\n\n\n181\n430\n493\n\n\n182\n314\n408\n\n\n183\n471\n476\n\n\n184\n399\n452\n\n\n185\n331\n391\n\n\n186\n454\n497\n\n\n187\n465\n558\n\n\n188\n397\n454\n\n\n189\n479\n572\n\n\n190\n456\n495\n\n\n191\n333\n408\n\n\n192\n300\n397\n\n\n193\n499\n583\n\n\n194\n306\n393\n\n\n195\n481\n555\n\n\n196\n483\n503\n\n\n197\n488\n528\n\n\n198\n385\n392\n\n\n199\n412\n435\n\n\n200\n471\n551\n\n\n\n\n\n\n\n\nggplot(patient_fup_data) +\n  geom_linerange(\n    aes(xmin = startFollowUp, xmax = endDeathCensor, y = ID)\n    , linewidth = 1) +\n  \n  theme_bw() +\n  \n  theme(\n    axis.text.x = element_text(size = 16, angle = 45),\n    strip.text = element_text(size = 18), \n    axis.title.x = element_text(size = 24),\n    axis.title.y = element_text(size = 24),\n    legend.position = 'right'\n  )\n\n\n\n\nSolution\nWe count in a rolling fashion. Note how multiple patients may enroll at the same time. Then if we want to know the total enroll at that unique entry time, we want to go to the end of the group line, where we have already accumulated everyone…\nOnce we slice, we lose some ID, so drop it. And, the start and end times are just for some arbitrary representative person, so drop them too. We just want the group statistics for the start of each unique enrollment or entry times.\n\nN_under_observation &lt;- \npatient_fup_data |&gt; \n  arrange(startFollowUp) |&gt; \n  \n  mutate(\n    Nenroll = row_number(), \n    Nlost = map_dbl(startFollowUp, \\(.x) sum(endDeathCensor &lt;= .x)), \n    N_obs  = Nenroll - Nlost \n  ) |&gt; \n  \n  \n  group_by(startFollowUp) |&gt; \n  slice(n()) \n\n\nggplot(N_under_observation, aes(x = startFollowUp, y = N_obs)) + \n  geom_step()\n\n\n\n\nHave we missed anything? Yes, ideally, we also want to capture exit times? But it makes the graph more choppy\n\nN_under_observation2 &lt;- \npatient_fup_data |&gt; \n  arrange(endDeathCensor) |&gt; \n  \n  mutate(\n    Nenroll = row_number(), \n    Nlost   = map_dbl(endDeathCensor, \\(.x) sum(endDeathCensor &lt;= .x)), \n    N_obs   = Nenroll - Nlost \n  ) |&gt; \n  \n  \n  group_by(startFollowUp) |&gt; \n  slice(n())"
  },
  {
    "objectID": "posts/gaps-and-islands/post.html",
    "href": "posts/gaps-and-islands/post.html",
    "title": "The Gaps and Islands Problem",
    "section": "",
    "text": "The gaps and islands problem arises from analyzing duration data as commonly captured in databases. Duration data are events recorded with a start datetime and end datetime. Rows in the table correspond to unique events that someone is experiencing, or during which times the event is active. As individuals ID may have one or more event activity, such data are often stored in long-form as date ranges or intervals.\n\n\n\n\n\n\nNote\n\n\n\nFor simplicity, we assume only one type of event under study — the data can be generalized to include an event-type field.\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\nset.seed(12345)\n\n# 10 individuals\nID &lt;- 1:10\n\n# 3 to 7 events per person \nn_events &lt;- sample(3:7, 10, replace = TRUE)\n\nduration_data &lt;- tibble(ID = rep(ID, n_events)) |&gt; \n  mutate(\n    start_date = \n      sample(seq(as.Date('2022-01-01'), as.Date('2022-12-31'), by = \"day\"), n(), replace = TRUE),\n    # 3 days - 2 months episodes \n    end_date = start_date + sample(3:60, n(), replace = TRUE)   \n  ) |&gt; \n  arrange(ID, start_date) |&gt; \n  group_by(ID) |&gt; \n  mutate(eventID = row_number())\n\n\nFor example:\n\nhead(duration_data, n=15) |&gt; kable()\n\n\n\n\nID\nstart_date\nend_date\neventID\n\n\n\n\n1\n2022-02-09\n2022-03-06\n1\n\n\n1\n2022-07-31\n2022-08-09\n2\n\n\n1\n2022-09-14\n2022-11-10\n3\n\n\n1\n2022-10-13\n2022-11-10\n4\n\n\n1\n2022-10-21\n2022-11-23\n5\n\n\n2\n2022-01-12\n2022-02-20\n1\n\n\n2\n2022-01-14\n2022-02-08\n2\n\n\n2\n2022-09-16\n2022-11-03\n3\n\n\n2\n2022-11-24\n2022-12-11\n4\n\n\n3\n2022-01-16\n2022-01-23\n1\n\n\n3\n2022-03-21\n2022-05-05\n2\n\n\n3\n2022-04-16\n2022-05-23\n3\n\n\n3\n2022-05-21\n2022-06-04\n4\n\n\n3\n2022-05-28\n2022-06-23\n5\n\n\n3\n2022-07-11\n2022-08-08\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo reveal the code how to simulate the demo data, click the Code text to expand above.\n\n\nHere, ID 1 experiences 5 distinct eventIDs, with the first one occurring on 2/9 and lasting for 25 days until 3/6. The key observation is that eventID 4 overlaps with eventID 5 within ID 1, where event 5 beginning on 10/21 starts before event 4 has ended on 11/10.\n\n\nVisualizing the date ranges of event occurrences is useful. We plot all individual event date ranges colored by ID, supplying a position_dodge so overlaps are revealed (instead of lying right on top one another).\n\n\nCode\nlibrary(ggplot2)\n\nggplot(duration_data |&gt; filter(ID &lt;= 5),\n       aes(y = factor(ID), xmin = start_date, xmax = end_date, color = factor(ID))) +\n  geom_linerange(position = position_dodge2(width = .5), linewidth = 2) +\n  theme_bw() + \n  xlab(\"Start/ End Dates\") + ylab(\"Individual ID\") +\n  scale_x_date(date_breaks = \"1 month\") + \n  theme(\n    axis.text.x = element_text(size = 16, angle = 45),\n    axis.text.y = element_text(size = 16),\n    axis.title.x = element_text(size = 24),\n    axis.title.y = element_text(size = 24),\n    legend.position = 'none'\n  )\n\n\n\n\n\nDuration patterns by subject\n\n\n\n\n\n\n\nGiven the set-up, how do we want to collapse the data? Our input data is structured as one row per date range per person. The desired output data is one row per coverage interval per person. By coverage, we mean a continuous block of time, defined by a start date and end date, during which the individual is experiencing one or more active events.\n\n\n\n\n\n\nNote\n\n\n\nCoverage intervals are referred to as islands. In-between time periods when there are no events occurring (no date ranges, overlapping or otherwise) are called gaps (holes).\n\n\nFor our demo data, within ID 2, there are 4 eventIDs, but just 3 coverage intervals (islands): some event is active between\n\n2022-01-12 through 2022-02-08 (where the date range of event 2 completely overlaps and within the duration time of event 1)\n2022-09-16 through 2022-11-03 (duration of eventID 3)\n2022-11-24 through 2022-12-11 (duration of last eventID 4)\n\nHence our desired output table will comprise 3 rows for ID 2, with start and end times as described above for each coverage interval. Likewise, by visual inspection, ID 3, who experienced 6 events, will have only 3 coverage intervals as well, with one particularly long coverage interval (of back-to-back events) beginning on 2022-03-21 and finally abating on 2022-06-23.\n\n\n\nExamining the gaps and islands solution can reveal hidden assumptions about data processes or quality issues you might not be aware of. For example, perhaps certain overlap patterns violate assumptions about event frequencies, or some duration lengths and patterns are not physically possible. Typically, the data captured does not reflect the complete story.\nConsider medical databases housing patient medication prescriptions. The table captures PatientID, DispensedDate, and DaysSupply, from which an EndDate is derived representing date of supply depletion relative to dispense time. The data as entered by pharmacists or business administrators for billing purposes or inventory management is only one part of the story, especially if our goal is to understand how patients are treated.\nFor example, suppose we observe multiple overlapping prescriptions for the same medication type (overlapping date ranges), but perhaps differing dose levels. It is not clear whether the patient has been instructed to take both prescriptions simultaneously (effective dose increase), or perhaps to switch to the lower dose before finishing the other prescription. We have missing contextual information! And, if we observe overlapping date ranges among different medication types, we may find new or surprising treatment regimes warranting investigation.\nIf we know what we see is impossible, and we have high assurance of data validity (they were not entered erroneously), we investigate further. Perhaps, it turns out a subset of prescriptions were returned-to-stock and were never picked up by the patient, requiring a check on another data field that we were not aware was being captured. With data in the wild, as we’re removed further away from the source of data origination, incomplete code-books, inaccurate code-books, or non-existent code-books represent the norm.\n\n\n\n\n\n\nNote\n\n\n\nUnless we already know what to look for, generalizing how we describe event patterns can help reveal new information requiring follow-up."
  },
  {
    "objectID": "posts/gaps-and-islands/post.html#visualization",
    "href": "posts/gaps-and-islands/post.html#visualization",
    "title": "The Gaps and Islands Problem",
    "section": "",
    "text": "Visualizing the date ranges of event occurrences is useful. We plot all individual event date ranges colored by ID, supplying a position_dodge so overlaps are revealed (instead of lying right on top one another).\n\n\nCode\nlibrary(ggplot2)\n\nggplot(duration_data |&gt; filter(ID &lt;= 5),\n       aes(y = factor(ID), xmin = start_date, xmax = end_date, color = factor(ID))) +\n  geom_linerange(position = position_dodge2(width = .5), linewidth = 2) +\n  theme_bw() + \n  xlab(\"Start/ End Dates\") + ylab(\"Individual ID\") +\n  scale_x_date(date_breaks = \"1 month\") + \n  theme(\n    axis.text.x = element_text(size = 16, angle = 45),\n    axis.text.y = element_text(size = 16),\n    axis.title.x = element_text(size = 24),\n    axis.title.y = element_text(size = 24),\n    legend.position = 'none'\n  )\n\n\n\n\n\nDuration patterns by subject"
  },
  {
    "objectID": "posts/gaps-and-islands/post.html#what-do-we-wish-to-solve-for",
    "href": "posts/gaps-and-islands/post.html#what-do-we-wish-to-solve-for",
    "title": "The Gaps and Islands Problem",
    "section": "",
    "text": "Given the set-up, how do we want to collapse the data? Our input data is structured as one row per date range per person. The desired output data is one row per coverage interval per person. By coverage, we mean a continuous block of time, defined by a start date and end date, during which the individual is experiencing one or more active events.\n\n\n\n\n\n\nNote\n\n\n\nCoverage intervals are referred to as islands. In-between time periods when there are no events occurring (no date ranges, overlapping or otherwise) are called gaps (holes).\n\n\nFor our demo data, within ID 2, there are 4 eventIDs, but just 3 coverage intervals (islands): some event is active between\n\n2022-01-12 through 2022-02-08 (where the date range of event 2 completely overlaps and within the duration time of event 1)\n2022-09-16 through 2022-11-03 (duration of eventID 3)\n2022-11-24 through 2022-12-11 (duration of last eventID 4)\n\nHence our desired output table will comprise 3 rows for ID 2, with start and end times as described above for each coverage interval. Likewise, by visual inspection, ID 3, who experienced 6 events, will have only 3 coverage intervals as well, with one particularly long coverage interval (of back-to-back events) beginning on 2022-03-21 and finally abating on 2022-06-23."
  },
  {
    "objectID": "posts/gaps-and-islands/post.html#application-in-medicine",
    "href": "posts/gaps-and-islands/post.html#application-in-medicine",
    "title": "The Gaps and Islands Problem",
    "section": "",
    "text": "Examining the gaps and islands solution can reveal hidden assumptions about data processes or quality issues you might not be aware of. For example, perhaps certain overlap patterns violate assumptions about event frequencies, or some duration lengths and patterns are not physically possible. Typically, the data captured does not reflect the complete story.\nConsider medical databases housing patient medication prescriptions. The table captures PatientID, DispensedDate, and DaysSupply, from which an EndDate is derived representing date of supply depletion relative to dispense time. The data as entered by pharmacists or business administrators for billing purposes or inventory management is only one part of the story, especially if our goal is to understand how patients are treated.\nFor example, suppose we observe multiple overlapping prescriptions for the same medication type (overlapping date ranges), but perhaps differing dose levels. It is not clear whether the patient has been instructed to take both prescriptions simultaneously (effective dose increase), or perhaps to switch to the lower dose before finishing the other prescription. We have missing contextual information! And, if we observe overlapping date ranges among different medication types, we may find new or surprising treatment regimes warranting investigation.\nIf we know what we see is impossible, and we have high assurance of data validity (they were not entered erroneously), we investigate further. Perhaps, it turns out a subset of prescriptions were returned-to-stock and were never picked up by the patient, requiring a check on another data field that we were not aware was being captured. With data in the wild, as we’re removed further away from the source of data origination, incomplete code-books, inaccurate code-books, or non-existent code-books represent the norm.\n\n\n\n\n\n\nNote\n\n\n\nUnless we already know what to look for, generalizing how we describe event patterns can help reveal new information requiring follow-up."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "A collection of posts on data management principles using R for the aspiring programmer."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Sieving Data : A Quarto Blog",
    "section": "",
    "text": "Introduction to map functions (when vectorized operations are not enough)\n\n\n\n\n\nWith an application to the nearest point problem\n\n\n\n\n\n\nOct 11, 2032\n\n\n\n\n\n\n  \n\n\n\n\nThe Gaps and Islands Problem\n\n\n\n\n\nAlso known as the overlapping-date-ranges problem, where we study duration data to identify coverage intervals (or, holes)\n\n\n\n\n\n\nOct 4, 2032\n\n\n\n\n\n\n  \n\n\n\n\nTO-DO: Conditional Aggregations\n\n\n\n\n\nConditional aggregations, application in survival analysis\n\n\n\n\n\n\nOct 4, 2032\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/intro-purrr-map/post.html",
    "href": "posts/intro-purrr-map/post.html",
    "title": "Introduction to map functions (when vectorized operations are not enough)",
    "section": "",
    "text": "Functions in the tidyverse suite of libraries are typically vectorized. They operate on vector (or lists) arguments and return vector (list) outputs. When variable field names of a data.frame are used in a tidyverse transformation, the function call is operating on the entire column(s) as inputs. Under a successful tidyverse transformation, the function output must match the input list dimensions to modify the original data.frame appropriately.\n\n\n\n\n\n\nNote\n\n\n\nIn R, vectors are special cases of lists where all elements in the list are of the same atomic type (dbl, char, logical, etc.) — i.e., when they are not further R objects or other lists.\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\nset.seed(12345)\n\ndemo_data &lt;- \n  tribble(\n    ~ \"X\", ~ \"Y\", ~ \"Z\",\n    3, 8, 5,\n    2, 4, 7,\n    6, 3, 8,\n    1, 5, 2\n  )\n\n\nConsider the follow demo data:\n\nkable(demo_data)\n\n\n\n\nX\nY\nZ\n\n\n\n\n3\n8\n5\n\n\n2\n4\n7\n\n\n6\n3\n8\n\n\n1\n5\n2\n\n\n\n\n\n\n\nWhen we create a new column via mutate — here, sum of X and Y and X + 1, it’s tempting to think the operation is happening row-by-row. We may even be tempted to visualize the computation as proceeding one row at a time, and updating an accompanying placeholder column cell-by-cell with the new result.\n\ndemo_data |&gt; \n  mutate(\n    Sum    = X + Y, \n    Xplus1 = X + 1  ) |&gt; kable() \n\n\n\n\nX\nY\nZ\nSum\nXplus1\n\n\n\n\n3\n8\n5\n11\n4\n\n\n2\n4\n7\n6\n3\n\n\n6\n3\n8\n9\n7\n\n\n1\n5\n2\n6\n2\n\n\n\n\n\n\n\nBut this is the wrong mental model! And it can lead us astray in more complex scenarios. While it might well be the case that somewhere down-the-line, internal to the machine, the operations are happening element-by-element, it is more useful (and more appropriate from the functional perspective) to view the addition call as taking vector inputs and returning vector outputs, from which mutate will then use to modify the data.frame. The function calls happen separately, and mutate is not looping through the data.frame row-by-row.\n\n\n\n\n\n\nImportant\n\n\n\nIn the tidyverse, transformation of a data.frame is a sequence of operations over one or more entire columns, returning new output columns to augment or replace the original columns. This is what is meant by vectorized functions or vectorized operations — The correct mental model is that of column-wise operation, not as row-wise operations (as we may be tempted to perceive).\n\n\nVectorized functions are enough to accomplish most data management tasks. But sometimes, we need finer control and want to operate element-by-element along one or more columns simultaneously (while manipulating other fields). In other words, we want to loop, while perhaps still processing over other columns treated as vector inputs — We want to loop (iterate) our vectorized function!\n\nUsing map\nApplication: Threholding and Truncation\n\n\n\n\n\n\nNote\n\n\n\nThe purrr family of map functions provide iteration for functions that are already vectorized.\n\n\nApplication - thresholding, truncation\n\nlibrary(purrr)\n\nx &lt;- 1:100\nupper_limit &lt;- c(12, 10, 98, 31)\n\nmap(upper_limit, \\(.upper_limit) x[x &lt; .upper_limit])\n\n[[1]]\n [1]  1  2  3  4  5  6  7  8  9 10 11\n\n[[2]]\n[1] 1 2 3 4 5 6 7 8 9\n\n[[3]]\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75\n[76] 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97\n\n[[4]]\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\nmap_dbl(upper_limit, \\(.upper_limit) length(x[x &lt; .upper_limit]))\n\n[1] 11  9 97 30\n\nmap_dbl(upper_limit, \\(.upper_limit) mean(x[x &lt; .upper_limit]))\n\n[1]  6.0  5.0 49.0 15.5\n\n# set upper to 0 in data.frame ?\nlabs &lt;- tibble(x = x, y = x, z = x)\n\nmap_dfc(upper_limit, \\(.upper_limit) labs |&gt; mutate_all(~ if_else(.x  &gt; .upper_limit, 0, .x))) |&gt; View()\n\n\n# pretend labs data are all ONE PATIENT \n# cum means of values within the last 30 days??\n\n# x = index date, or visit dates \n# y = value date \n# z = value \n\nlabs |&gt; mutate(W = \n  map_dbl(x, \\(.x) mean(z[y &lt;= .x & y &gt;= .x - 30]))\n  )\n\n# A tibble: 100 × 4\n       x     y     z     W\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     1     1     1   1  \n 2     2     2     2   1.5\n 3     3     3     3   2  \n 4     4     4     4   2.5\n 5     5     5     5   3  \n 6     6     6     6   3.5\n 7     7     7     7   4  \n 8     8     8     8   4.5\n 9     9     9     9   5  \n10    10    10    10   5.5\n# ℹ 90 more rows\n\n# or same values \n\nlabs |&gt; mutate(W = \n  map_dbl(x, \\(.x) mean(z[y &lt;= .x & y &gt;= .x - 30]))\n  )\n\n# A tibble: 100 × 4\n       x     y     z     W\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     1     1     1   1  \n 2     2     2     2   1.5\n 3     3     3     3   2  \n 4     4     4     4   2.5\n 5     5     5     5   3  \n 6     6     6     6   3.5\n 7     7     7     7   4  \n 8     8     8     8   4.5\n 9     9     9     9   5  \n10    10    10    10   5.5\n# ℹ 90 more rows\n\n\n\n\nPatient look-back window\nConsider the following application. In patient medical trials, a variety of vital sign measurements are taken. Suppose we have a cohort of patients, that are coming in for regularly schedule follow-up visits. At each visit, vital signs X, Y, and Z are drawn. However, it turns out that sometimes patients miss a scheduled appointment, so the intervals are not evenly-spaced. Moreover, at certain occasions, a particular vital sign measurement is either forgotten to be taken, or unable to be performed due to technical difficulties. Then the data may look like as follows:\n\npatient_visit_data &lt;- tibble(\n  ID        = rep(1:10, 10),\n  VisitDate = sample(seq(as.Date('2022-01-01'), as.Date('2022-06-30'), by = \"day\"), 100, replace=TRUE), \n  X         = rnorm(100, 30, 10),\n  Y         = rnorm(100, 83, 20),\n  Z         = rnorm(100, 50, 5) \n  ) |&gt; \n  \n  # assume 30% missing rate for each variable\n  mutate_at(\n    vars(X,Y,X), \n    \\(.x) if_else(sample(x=0:1, size=100, replace=TRUE, prob = c(.70,.30)) == 1, NA, .x)) |&gt; \n  \n  arrange(ID, VisitDate) |&gt; \n  \n  mutate_if(is.numeric, round)  \n\nNow, we want to compute and track some composite score, at each visit. We use the value taken, but willing to impute with the closest value 30 days prior. One composite measure is defined as simply \\(X + Y\\), and the other is \\(X + Y - Z\\), where for the later, we require all 3 to within the last 30 days relative to visit date, which is what we are using as index date for score computation peg.\n\npatient_visit_data |&gt; \n  group_by(ID) |&gt; \n  mutate(\n    XplusY = map_dbl(VisitDate, \\(.visitdate) \n      last(X[VisitDate &lt;= .visitdate & VisitDate &gt;= .visitdate - 30], na_rm = T) +\n      last(Y[VisitDate &lt;= .visitdate & VisitDate &gt;= .visitdate - 30], na_rm = T))) |&gt; \n  \n  kable(n = 20)\n\n\n\n\nID\nVisitDate\nX\nY\nZ\nXplusY\n\n\n\n\n1\n2022-02-07\nNA\nNA\n43\nNA\n\n\n1\n2022-02-26\n34\n66\n41\n100\n\n\n1\n2022-03-13\n28\n101\n54\n129\n\n\n1\n2022-03-15\n32\nNA\n48\n133\n\n\n1\n2022-03-21\nNA\n82\n38\n114\n\n\n1\n2022-03-31\n38\n70\n44\n108\n\n\n1\n2022-04-19\nNA\n112\n43\n150\n\n\n1\n2022-05-17\n18\n71\n56\n89\n\n\n1\n2022-05-22\n35\n93\n48\n128\n\n\n1\n2022-06-26\n46\n100\n50\n146\n\n\n2\n2022-01-12\n24\n92\n48\n116\n\n\n2\n2022-01-15\n34\n113\n47\n147\n\n\n2\n2022-02-20\nNA\n103\n50\nNA\n\n\n2\n2022-03-03\nNA\nNA\n53\nNA\n\n\n2\n2022-03-15\nNA\n93\n48\nNA\n\n\n2\n2022-04-13\nNA\nNA\n52\nNA\n\n\n2\n2022-05-04\n9\nNA\n55\nNA\n\n\n2\n2022-05-13\n31\nNA\n46\nNA\n\n\n2\n2022-06-25\n28\n88\n43\n116\n\n\n2\n2022-06-28\n44\nNA\n59\n132\n\n\n3\n2022-01-03\n39\nNA\n53\nNA\n\n\n3\n2022-01-07\nNA\n105\n53\n144\n\n\n3\n2022-02-24\n16\n78\n49\n94\n\n\n3\n2022-02-25\n35\n109\n50\n144\n\n\n3\n2022-03-27\n48\nNA\n40\n157\n\n\n3\n2022-04-04\n30\n94\n47\n124\n\n\n3\n2022-04-08\n37\n63\n55\n100\n\n\n3\n2022-04-09\n28\nNA\n52\n91\n\n\n3\n2022-05-25\n36\n103\n60\n139\n\n\n3\n2022-06-01\nNA\n89\n50\n125\n\n\n4\n2022-01-05\n38\n105\n56\n143\n\n\n4\n2022-01-10\n21\n70\n50\n91\n\n\n4\n2022-01-23\n38\n77\n47\n115\n\n\n4\n2022-02-11\nNA\n90\n54\n128\n\n\n4\n2022-02-24\n9\n87\n43\n96\n\n\n4\n2022-02-27\n30\n115\n52\n145\n\n\n4\n2022-04-17\n31\n30\n58\n61\n\n\n4\n2022-05-03\nNA\nNA\n55\n61\n\n\n4\n2022-05-07\n28\n73\n51\n101\n\n\n4\n2022-05-17\n45\n65\n53\n110\n\n\n5\n2022-01-14\nNA\n46\n42\nNA\n\n\n5\n2022-03-01\n34\n55\n50\n89\n\n\n5\n2022-03-31\n26\n96\n48\n122\n\n\n5\n2022-04-03\n27\n97\n54\n124\n\n\n5\n2022-04-05\n35\n72\n51\n107\n\n\n5\n2022-04-17\nNA\nNA\n53\n107\n\n\n5\n2022-05-01\nNA\n120\n50\n155\n\n\n5\n2022-05-25\nNA\nNA\n48\n124\n\n\n5\n2022-05-25\n23\n101\n50\n124\n\n\n5\n2022-06-09\n27\n87\n46\n114\n\n\n6\n2022-01-03\n40\n76\n52\n116\n\n\n6\n2022-02-09\nNA\nNA\n52\nNA\n\n\n6\n2022-02-24\n28\n76\n58\n104\n\n\n6\n2022-03-09\n40\n101\n57\n141\n\n\n6\n2022-03-16\nNA\n89\n49\n129\n\n\n6\n2022-04-01\n47\n78\n48\n125\n\n\n6\n2022-04-15\n35\nNA\n45\n113\n\n\n6\n2022-05-21\n42\n104\n44\n146\n\n\n6\n2022-06-03\n25\n79\n40\n104\n\n\n6\n2022-06-14\n32\nNA\n49\n111\n\n\n7\n2022-01-18\n35\n91\n55\n126\n\n\n7\n2022-02-01\n24\n44\n46\n68\n\n\n7\n2022-02-27\n35\n73\n42\n108\n\n\n7\n2022-04-06\n26\n90\n56\n116\n\n\n7\n2022-04-26\n24\n102\n51\n126\n\n\n7\n2022-05-01\nNA\n88\n42\n112\n\n\n7\n2022-05-12\nNA\n92\n48\n116\n\n\n7\n2022-05-15\n56\n90\n60\n146\n\n\n7\n2022-05-28\nNA\n64\n54\n120\n\n\n7\n2022-06-16\nNA\n89\n46\nNA\n\n\n8\n2022-01-02\nNA\nNA\n47\nNA\n\n\n8\n2022-01-11\n24\n74\n56\n98\n\n\n8\n2022-01-25\n28\nNA\n41\n102\n\n\n8\n2022-02-03\n40\n93\n59\n133\n\n\n8\n2022-02-07\nNA\nNA\n50\n133\n\n\n8\n2022-04-16\n5\n66\n52\n96\n\n\n8\n2022-04-16\n30\nNA\n49\n96\n\n\n8\n2022-05-12\n42\n93\n49\n135\n\n\n8\n2022-05-15\nNA\n102\n52\n144\n\n\n8\n2022-05-23\n27\nNA\n46\n129\n\n\n9\n2022-01-13\n44\n106\n44\n150\n\n\n9\n2022-01-30\n44\n122\n46\n166\n\n\n9\n2022-02-05\nNA\n77\n51\n121\n\n\n9\n2022-02-15\n11\n98\n48\n109\n\n\n9\n2022-02-18\n22\n97\n54\n119\n\n\n9\n2022-03-27\nNA\n87\n43\nNA\n\n\n9\n2022-04-05\n36\n110\n44\n146\n\n\n9\n2022-04-19\n32\nNA\n57\n142\n\n\n9\n2022-04-29\n38\nNA\n60\n148\n\n\n9\n2022-06-21\n21\n70\n48\n91\n\n\n10\n2022-01-01\n40\n81\n50\n121\n\n\n10\n2022-01-16\n29\n60\n54\n89\n\n\n10\n2022-02-01\n41\n99\n47\n140\n\n\n10\n2022-03-08\n27\n58\n46\n114\n\n\n10\n2022-03-08\n8\n106\n50\n114\n\n\n10\n2022-03-16\n33\n99\n48\n132\n\n\n10\n2022-05-20\nNA\nNA\n43\nNA\n\n\n10\n2022-05-28\n42\nNA\n57\nNA\n\n\n10\n2022-06-12\nNA\nNA\n45\nNA\n\n\n10\n2022-06-15\nNA\n60\n46\n102\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe function last has an argument called na_rm with default value False. Here, we want the last available measurement recorded when sorted by descending date — which will correspond to the most recent record.\n\n\nFor example, one column act as a list of arguments or parameters, and we want to use the current value as a scalar input to aggregate over another column taken a list type.\nInstead, consider the following task: Create a new column W where for each value of X, we sum all values of Y but after having multiplied them by the current X scalar value.\nWe can get this to work by hand:\n\ndemo_data$X[1]  \n\n[1] 3\n\ndemo_data$X[1] * demo_data$Y\n\n[1] 24 12  9 15\n\nsum(demo_data$X[1] * demo_data$Y) \n\n[1] 60\n\n\nIf our brains continue to think things work row-by-row, we may be led astray with an incorrect first attempt:\n\ndemo_data |&gt; \n  mutate(\n    W = sum(X * Y)) |&gt; kable() \n\n\n\n\nX\nY\nZ\nW\n\n\n\n\n3\n8\n5\n55\n\n\n2\n4\n7\n55\n\n\n6\n3\n8\n55\n\n\n1\n5\n2\n55\n\n\n\n\n\n\n\nBecause W return type must be a list, but the function operations are aggregating over other list types, it just repeats the same value along each row. But we don’t want it to act on X as a vector.\n\n\n\n\n\n\nTip\n\n\n\nA scalar is a one-dimensional data type (compared to matrices, lists, data.frames, objects, etc.). Simply, a scalar is a singular value or number, and that’s all — 2 is scalar, but the vector containing the single element two, [2], is not.\n\n\nIntroduce map_dbl. The first argument is a list, and the second argument is a function of the scalar value extracted along the list, and along with other data lists taken from the data.frame environment. Hence it will return a new vector, one dbl result for processing each value in the list input, and this satisfies the return type that mutate is returning an entire vector output matching the dimensions of the original data.frame\n\ndemo_data |&gt; \n  mutate(\n    W = map_dbl(X, \\(.x) sum(.x * Y))) |&gt; kable() \n\n\n\n\nX\nY\nZ\nW\n\n\n\n\n3\n8\n5\n60\n\n\n2\n4\n7\n40\n\n\n6\n3\n8\n120\n\n\n1\n5\n2\n20\n\n\n\n\n\n\n\nOur anonymous function uses the entire Y list as input to it’s processing. The .x argument must be a scalar because it is drawn from X within the use of map_dbl, which will run the function for each value of X, putting the result into a return list W\n\n\nWarm-up: Leave-one-out comparisons\nWithin group, compare to means of the others…\nLet’s generate some data by group, where maybe observations different from the rest. drawn from a mixture of distributions of sort.\n\n# 100 rnorm \nx &lt;- rnorm(90)\ny &lt;- rep(5, 10)\n\ndemo_grp_data &lt;-\n  tibble(\n    ID = rep(1:10, 10),\n    v = sample(c(x,y), replace = F)\n  )\n\nFor each observation within ID, we want to compute the absolute difference between the observation under consideration, with the mean of the rest of the observations in the group.\n\ndemo_grp_data |&gt; \n  group_by(ID) |&gt; \n  mutate(\n    X = map_dbl(v, \\(.x) (sum(v) - .x)/(length(v) - 1)) \n  ) |&gt; \n  \n  arrange(ID) \n\n# A tibble: 100 × 3\n# Groups:   ID [10]\n      ID      v     X\n   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1     1  1.39   2.06\n 2     1  5      1.66\n 3     1  5      1.66\n 4     1  5      1.66\n 5     1  0.375  2.18\n 6     1 -0.943  2.32\n 7     1  0.625  2.15\n 8     1 -0.777  2.30\n 9     1 -0.721  2.30\n10     1  5      1.66\n# ℹ 90 more rows"
  }
]